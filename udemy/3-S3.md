# S3 (simple storage service)

One of the oldest service of AWS.

- Object-based storage. 
- It is stored across multiple devices (redundancy)

## Basics of S3

- Object-based
- Files can be from 0 Bytes to 5 TB
- Unlimited Storage
- Files are stored in buckets

Bucket is just a folder

- S3 is a universal namespace. (names must be unique globally). Bucket gets a web-address.
- When you upload a file to S3, you will receive a HTTP 200 code if the upload was successful.

Objects consist of the following :

- Key (This is simply the name of the object)
- Value (the data, made up of a sequence of bytes)
- Version ID (important for versioning)
- Metadata (data about data you are storing e.g. notes)
- Subresources like Access Control Lists and Torrents.

## How does Data Consistency work for S3?

- Read after Write consistency for PUTS of new objects (you can read your file right after you put the file into the bucket)
- Eventual consistency for overwrite PUTS (update) and DELETES (can take some time to propagate) (if you had version 1, and updated to version 2, immediate read can show version 1 or version 2. What S3 promises is that it will always show version 2 file after some amount of time)

## What does S3 guarantee?

- 99.9% availability
- **Eleven nine's** durability (data will not be lost) 99.9999999%

## Following Features

- Tiered Storage Available (So important)
- Life cycle management (e.g. after 30 days move the data to Glacier)
- Versioning
- Encryption
- MFA Delete (hardens deletion)
- Secure your data using Access Control Lists and Bucket Policies

## There are many classes in S3 Storage

### S3 standard

99.99% Availability and eleven 9s durability (achieved by redundancy). It is designed to sustain the loss of 2 facilities concurrently (which may never happen)

### S3 IA (infrequently accessed)

For data that is access less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee.

### S3 One Zone - IA (Reduce Redundancy Storage)

Lower cost option of IA

- IMPORTANT : RRS is not an alias for 1 zone - IA. It is a product that amazon doesn't want to sell anymore. RRS is deprecated ! ! ! 

### S3- intelligent tiering

Designed to optimize costs by automatically moving data to the most cost-effective access tier. (Machine Learning)

### S3 Glacier

This is used for archiving. Secure, Durable, Low-cost storage class. This is always cheaper than on-premises solutions. Cheapest. But it take minutes to hours to retrieve.

### S3 Glacier Deep Archive

LOWEST COST, but take 12 hours to retrieve data.

### In summary,,,

First byte latency usually shows the retrieval time. It is usually milliseconds, except for glacier (minutes or hours), and glacier deep archive(12 hours)

Durability is fixed to 11 9s.

## Charged for S3 in the following ways

- Storage
- Requests
- Storage Management Pricing
- Data Transfer Pricing
- Transfer acceleration
- Cross region replication

## What the hell is cross region replication and transfer acceleration?

For disaster recovery and high availability. If cross region replication is turned on, the bucket in us-east 1 would automatically replicated into some other regions too.

Transfer Acceleration could be done by taking advantage of Amazon's cloud fronts (edge location). It is a fast, secure transfer of files over long distances between your end users and an S3 bucket.

Use case of transfer acceleration is as follows : Consider a case that you own a bucket, and users from global uploads their files to your buckets. If they upload their files all over the world, they are uploading their files to the edge locations **not to the S3 buckets itself**, then the **amazon backbone network** which connects the edge location and your own bucket, and then the data would finally go into your S3 bucket. It boosts the uploading time.

## Tricky Tips

- Buckets are a universal name space.
- S3 is not suitable service for installing an OS or server on. For these cases, block based storage would be more appropriate. You don't want to install an OS on S3 and use it to host a database.
- If the upload was successful, then you'll get a HTTP 200 status code.
- MFA delete
- Read after Write consistency for PUTS of new objects
- Eventual consistency for updating and deleting
- Numerous storage levels (e.g. standard, intelligent tiering, ... ) could also be applied to a single object.

## Encryption of S3 bucket

By default, newly created buckets are PRIVATE. You can setup access control to your buckets using 1. Bucket Policies (bucket level), 2. Access Control Lists (individual level)

If you want, you can create access logs which log all request made to the S3 bucket. This log can be sent to another bucket or even another bucket in another account!

## Basics of Encryption in S3

There are two types of Encryption.

1. Encryption in transit (such as HTTPS) is achieved by SSL/TLS (the connection itself is encrypted)
2. Encryption at rest (server side) (this means that server or the client is responsible for encrypting the file, but we will look for server side encryption for now)

At rest encryption can be achieved by three ways

- S3 Managed Keys (SSE - S3) : Amazon manage all the keys
- AWS Key Management Service, Managed Keys (SSE-KMS) : You and Amazon manage the keys both
- Server Side encryption with Customer provided keys (SSE-C) : You are just giving your keys to Amazon which you manage by yourself.

3. Client Side Encryption (not very interesting)

So if I encrypted a certain object by AES-256, then even a malicious person who took the hard disk drive from the amazon data center couldn't decrypt this file.

## Using versioning with S3

- Stores all versions of an object (including all writes and even if you **delete** an object)
- Great backup tool
- Only enabled, only suspended. Cannot be disabled!
- Integrated with Life cycle rules
- **Versioning's MFA delete capability**


The size of bucket (enabled versioning) would be the sum of all the version files. e.g. sample.txt was 23B and updated into 43B and then updated to 20B, then all the contents are now being saved and the size of the bucket would be 86B. DO NOT confuse with EBS snapshot (incremental saving)

So, if you turn versioning on, it could cost an exponential amount of money. So you may need to enable life cycle policy to retire old versions

- Also note that uploading object is always private in default. (Even if the previous one was set to be public)
- If you deleted the object, it could be restored by deleting the delete marker (would be the latest version)
- As we can infer, we can delete individual version. 

## Life Cycle Management 

It automates moving your objects between the different storage tiers.
Can be used in conjunction with versioning

- We can manage objects for current version and for the older version separately. 
- Transitions : e.g. transit the objects to glacier if 60 days have been past after creation.
- Expiration : e.g. expire the object after 425 days from object creation.

## Cross Region Replication

- In order to make bucket enable Cross Region Replication, Versioning should be turned on first.
- Versioning must be enabled on both the source and destination buckets.
- You can whether replicate the whole bucket or select some objects with certain tags. And after selecting the destination region, new bucket will be generated (or you can choose the pre-existing bucket for the destination). 
- The prior contents in the source bucket would not be in the new bucket automatically. But the new bucket inherits the prior bucket policy.
- All subsequent updated files will be replicated automatically.
- **Delete marker is not replicated** in order to halt people from accidentally delete objects.
- Subsequently, deleting individual versions or delete markers will not be replicated.

## Transfer Acceleration

So it utilises the Cloud Front Edge Network to accelerate your uploads to S3. You will use a **distinct URL** to upload directly to an edge location which will then transfer that file to S3 (by backbone network of AWS).

# Cloud Front (only briefly)

Distributed Network of servers which delivers web pages and content to users based on their geographical location, the origin of the web page and a content delivery server.


- Edge Location : Location where content will be cached (separate to an AWS Region/ AZ)
- Origin : All the files that the Content Delivery Network will distribute (could be an S3 bucket, EC2 instance, ELB,,...)
- Distribution : Collection of Edge locations (name given the CDN)

Two types of distribution

1. Web Distribution
2. RTMP (used for video streaming, streaming media files using Adobe Flash Media Server's RTMP protocol)

Edge locations are not just READ only - you can write or put an object on to them.

Objects are cached for the life of the TTL (time to live)

You can clear cached objects, but you will be charged (**a.k.a. invalidation**) (when you changed your video, but the users all over the globe would see the old one. You may clear the cached objects)

And also, remember that you can restrict viewer access using signed URLs(temporary) or signed cookies(just set it once, then can have access to multiple restricted files)


**Remember creating invalidation**

# Snowball?

Snowball is a petabyte-scale data transport solution to transfer large amounts of data into and out of AWS. Transferring data with Snowball is simple, fast, secure, and cheap

## Features of Snowball

- comes in either a 50TB or 80TB size.
- Multiple layers of security including tamper-resistant enclosures, 256-bit encryption, and an industry-standard Trusted Platform Module (TPM) designed to ensure both security and full chain-of-custody of your data.
- Once the data transfer job has been verified, AWS performs a software erasure of the snowball appliance. So nobody can just take a snowball and restore the data from previous customers. It is just nuked as hell.

## Features of Snowball Edge

- Snowball Edge is a 100TB data transfer device with on-board storage and **compute capabilities**. Three use cases : move large amounts of data into and out of AWS, temporary storage tier for large local datasets, to support local workloads in remote or off-line locations (works well with AWS lambda !)
- Snowball Edge can cluster together to form a local storage tier and process your data on-premises. This helps ensuring your applications continue to run even when they are not able to access the cloud. (it's like having a portable AWS)

## Features of Snowmobile

Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. Makes it easy to move massive volumes of data to the cloud (even complete data center!)

# Storage Gateway

This service enables you to securely store the data into the AWS cloud. Suppose you have your own data center and want to transfer the content to AWS. Then you might have a storage gateway at your side (can be manifested either in physical or in virtual device). And that would replicate your data to AWS.

Storage Gateway's Software appliance is available for you to download as a virtual machine image that you install on a host in your data center. Then, associate it with the AWS account through activation process.

## Three different types of Storage Gateway

### File Gateway (NFS & SMB) <Ways to store files in S3>

Files are stored as objects in your S3 buckets, accessed through a Network File System(NFS) mount point. Once the files are in the S3 bucket, it is treated exactly as native S3 objects.

### Volume Gateway (iSCSI) <Ways to store copies of your hard disk drives or virtual drives in S3>

- iSCSI stands for Internet Small Computer System Interface
- The volume gateway interface presents your applications with disk volumes using the iSCSI block protocol.
- Data written to this volume is stored in the cloud as Amazon EBS snapshot. **and also can be asynchronously backed up as point-in-time snapshots of your volumes**
- Snapshots are incremental, and they are also compressed.

#### Stored Volumes

Let you store **all the data** locally.

- So it is used when low latency access to the entire data set is needed.
- Backup method : Asynchronous point-in-time snapshots of this data to S3.

#### Cached Volumes 

It only deals with the most frequently accessed data.

- Minimize the need to scale your on-premises storage infrastructure
- substantial cost savings

### Tape Gateway (Virtual Tape Library)

Offers a durable, cost-effective solution

Good for storing our backup data to amazon cloud.

Replicates our virtual tapes to S3.

- Cloud-backed virtual tape storage
- With a tape gateway, you can cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE

## Storage Gateway/Tips

File Gateway : For flat files, stored directly on S3.

Volume Gateway 

1. Stored Volumes :  Entire Data is stored on site and is asynchronously backed up to S3
2. Cached Volumes :  **Entire Data is stored on S3** and the most frequently accessed data is cached on site

- Note : What does it mean by cached/stored on site? It means it is possible to retain the data in your storage gateway itself.

Virtual/physical storage gateway appliance is both available.
